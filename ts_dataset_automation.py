# -*- coding: utf-8 -*-
"""TS_DATASET_AUTOMATION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bNpiaTR5AJ-R1zYouSxeQiRLTPv6h6dV
"""

!pip install wget
import gzip
import shutil
import os
import wget
import csv
import linecache
from shutil import copyfile
import ipywidgets as widgets
import numpy as np
import pandas as pd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
from pandas import datetime as dt
from matplotlib import pyplot
from statsmodels.tsa.arima_model import ARIMA
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import datetime

##########################################################################################################################
#                                                                                                                        #
#                                                      Get File Name Functions                                           #
#                                                                                                                        #
##########################################################################################################################
from urllib.parse import urlparse
def get_filename(url):
    a=urlparse(url)
    fname=os.path.basename(a.path)
    return os.path.splitext(fname)[0]

def get_filenameext(url):
    a=urlparse(url)
    fname=os.path.basename(a.path)
    return fname

def readFile(fileName):
    fileObj = open(fileName, "r") #opens the file in read mode
    words = fileObj.read().splitlines() #puts the file into an array
    fileObj.close()
    return words

##########################################################################################################################
#                                                                                                                        #
#                                                      Get Valid/Updated URLs                                            #
#                                                                                                                        #
##########################################################################################################################
import requests
from array import array
with open("lastdate.txt", "r") as text_file:
  lastdate = text_file.read()
text_file.close()


start = datetime.datetime.strptime(lastdate, "%Y-%m-%d")
end = datetime.datetime.strptime("2021-10-31", "%Y-%m-%d")
date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days)]


for date in date_generated:
  surl="https://github.com/thepanacealab/covid19_twitter/blob/master/dailies/"+date.strftime("%Y-%m-%d")+"/"+date.strftime("%Y-%m-%d")+"-dataset.tsv.gz?raw=true"

  try:
    r = requests.get(surl)
    if r.status_code==200:
      with open("dataurl.txt", "a") as data_file:
        data_file.write(surl+'\n')
      data_file.close()
      
    else:
      print(date.strftime("%Y-%m-%d") + "\tNA FAILED TO CONNECT\t")
      with open("lastdate.txt", "w") as text_file:
        text_file.write(date.strftime("%Y-%m-%d"))
      text_file.close()
      break

  except Exception as e:
    break

##########################################################################################################################
#                                                                                                                        #
#                                                Download/Extract/Remove Files                                           #
#                                                                                                                        #
##########################################################################################################################
dataArray=readFile('dataurl.txt')
for x in dataArray:

    dataset_URL = x
    fname=get_filename(dataset_URL)
    fnameext=get_filenameext(dataset_URL)

    #Downloads the dataset (compressed in a GZ format)
    #!wget dataset_URL -O clean-dataset.tsv.gz
    wget.download(dataset_URL, out=fnameext)

    #Unzips the dataset and gets the TSV dataset
    with gzip.open(fnameext, 'rb') as f_in:
        with open(fname, 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)

    #Deletes the compressed GZ file
    os.unlink(fnameext)

dataArray=readFile('dataurl.txt')
colnames= ('tweet_id','date','time','lang','country_code')
df5 = pd.DataFrame(columns = colnames)
df4 = pd.DataFrame()
for x in dataArray:
    dataset_URL = x

    fname=get_filename(dataset_URL)
    fnameext=get_filenameext(dataset_URL)
    df3 = pd.read_csv(fname,sep="\t")
    df3 = df3[df3['country_code'].notna()]
    df5=df5.append(df3,True)

df5.tail()

!pip install mysqlclient
from sqlalchemy import create_engine
my_conn = create_engine("mysql+mysqldb://RWZCore7jN:2hzjIzS4gT@remotemysql.com/RWZCore7jN")

df5.to_sql(con=my_conn,name='ts_data',if_exists='append',index=False)